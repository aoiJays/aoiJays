<!DOCTYPE html>
<html lang="zh">
    <!-- title -->
<!-- keywords -->
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="author" content="BiribiriBird">
    <meta name="renderer" content="webkit">
    <meta name="copyright" content="BiribiriBird">
        <meta name="keywords" content="hexo,hexo-theme,hexo-blog,Neworld1111,AoiJays,BiribiriBird">
    <meta name="description" content="">
    <meta name="description" content="Pytorch入门   环境配置 前言  法宝函数 Dataset Tensorboard  Scalars Images   Transforms Torchvision数据集的下载与使用 DataLoader   网络  卷积层 池化层 激活层 其他 Sequential   损失函数 反向传播 优化器 模型的修改  添加 修改   模型的保存与读取  save state_dict（官方推荐">
<meta property="og:type" content="article">
<meta property="og:title" content="Pytorch小土堆 - 笔记">
<meta property="og:url" content="https://aoijays.top/2024/07/21/Pytorch-intro/index.html">
<meta property="og:site_name" content="BiribiriBird&#39;s Gallery">
<meta property="og:description" content="Pytorch入门   环境配置 前言  法宝函数 Dataset Tensorboard  Scalars Images   Transforms Torchvision数据集的下载与使用 DataLoader   网络  卷积层 池化层 激活层 其他 Sequential   损失函数 反向传播 优化器 模型的修改  添加 修改   模型的保存与读取  save state_dict（官方推荐">
<meta property="og:locale">
<meta property="og:image" content="https://aoijays.top/pic/Pytorch_%E5%B0%8F%E5%9C%9F%E5%A0%86/image-20240529020547606.png">
<meta property="og:image" content="https://aoijays.top/pic/Pytorch_%E5%B0%8F%E5%9C%9F%E5%A0%86/image-20240529030116600.png">
<meta property="og:image" content="https://aoijays.top/pic/Pytorch_%E5%B0%8F%E5%9C%9F%E5%A0%86/image-20240529031540127.png">
<meta property="og:image" content="https://aoijays.top/pic/Pytorch_%E5%B0%8F%E5%9C%9F%E5%A0%86/image-20240530152550478.png">
<meta property="article:published_time" content="2024-07-21T01:50:19.000Z">
<meta property="article:modified_time" content="2024-07-28T07:53:46.617Z">
<meta property="article:author" content="BiribiriBird">
<meta property="article:tag" content="深度学习">
<meta property="article:tag" content="Pytorch">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://aoijays.top/pic/Pytorch_%E5%B0%8F%E5%9C%9F%E5%A0%86/image-20240529020547606.png">
    <meta http-equiv="Cache-control" content="no-cache">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    <link rel="icon" href="/assets/favicon.ico">
    <title>Pytorch小土堆 - 笔记 · BiribiriBird&#39;s Gallery</title>
    <!-- /*! loadCSS. [c]2017 Filament Group, Inc. MIT License */
/* This file is meant as a standalone workflow for
- testing support for link[rel=preload]
- enabling async CSS loading in browsers that do not support rel=preload
- applying rel preload css once loaded, whether supported or not.
*/ -->
<script>
    (function (w) {
        'use strict'
        // rel=preload support test
        if (!w.loadCSS) {
            w.loadCSS = function () {}
        }
        // define on the loadCSS obj
        var rp = (loadCSS.relpreload = {})
        // rel=preload feature support test
        // runs once and returns a function for compat purposes
        rp.support = (function () {
            var ret
            try {
                ret = w.document.createElement('link').relList.supports('preload')
            } catch (e) {
                ret = false
            }
            return function () {
                return ret
            }
        })()

        // if preload isn't supported, get an asynchronous load by using a non-matching media attribute
        // then change that media back to its intended value on load
        rp.bindMediaToggle = function (link) {
            // remember existing media attr for ultimate state, or default to 'all'
            var finalMedia = link.media || 'all'

            function enableStylesheet() {
                link.media = finalMedia
            }

            // bind load handlers to enable media
            if (link.addEventListener) {
                link.addEventListener('load', enableStylesheet)
            } else if (link.attachEvent) {
                link.attachEvent('onload', enableStylesheet)
            }

            // Set rel and non-applicable media type to start an async request
            // note: timeout allows this to happen async to let rendering continue in IE
            setTimeout(function () {
                link.rel = 'stylesheet'
                link.media = 'only x'
            })
            // also enable media after 3 seconds,
            // which will catch very old browsers (android 2.x, old firefox) that don't support onload on link
            setTimeout(enableStylesheet, 3000)
        }

        // loop through link elements in DOM
        rp.poly = function () {
            // double check this to prevent external calls from running
            if (rp.support()) {
                return
            }
            var links = w.document.getElementsByTagName('link')
            for (var i = 0; i < links.length; i++) {
                var link = links[i]
                // qualify links to those with rel=preload and as=style attrs
                if (
                    link.rel === 'preload' &&
                    link.getAttribute('as') === 'style' &&
                    !link.getAttribute('data-loadcss')
                ) {
                    // prevent rerunning on link
                    link.setAttribute('data-loadcss', true)
                    // bind listeners to toggle media back
                    rp.bindMediaToggle(link)
                }
            }
        }

        // if unsupported, run the polyfill
        if (!rp.support()) {
            // run once at least
            rp.poly()

            // rerun poly on an interval until onload
            var run = w.setInterval(rp.poly, 500)
            if (w.addEventListener) {
                w.addEventListener('load', function () {
                    rp.poly()
                    w.clearInterval(run)
                })
            } else if (w.attachEvent) {
                w.attachEvent('onload', function () {
                    rp.poly()
                    w.clearInterval(run)
                })
            }
        }

        // commonjs
        if (typeof exports !== 'undefined') {
            exports.loadCSS = loadCSS
        } else {
            w.loadCSS = loadCSS
        }
    })(typeof global !== 'undefined' ? global : this)
</script>

    <style type="text/css">
    @font-face {
        font-family: 'Oswald-Regular';
        src: url("/font/Oswald-Regular.ttf");
    }

    body {
        margin: 0;
    }

    header,
    footer,
    .footer-fixed-btn,
    .sidebar,
    .container,
    .site-intro-meta,
    .toc-wrapper {
        display: none;
    }

    .site-intro {
        position: relative;
        z-index: 3;
        width: 100%;
        /* height: 50vh; */
        overflow: hidden;
    }

    .site-intro-placeholder {
        position: absolute;
        z-index: -2;
        top: 0;
        left: 0;
        width: calc(100% + 300px);
        height: 100%;
        background: repeating-linear-gradient(
            -45deg,
            #444 0,
            #444 80px,
            #333 80px,
            #333 160px
        );
        background-position: center center;
        transform: translate3d(-226px, 0, 0);
        animation: gradient-move 2.5s ease-out 0s infinite;
    }

    @keyframes gradient-move {
        0% {
            transform: translate3d(-226px, 0, 0);
        }
        100% {
            transform: translate3d(0, 0, 0);
        }
    }
</style>

    <link id="stylesheet-fancybox" rel="preload" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.36/dist/fancybox/fancybox.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
    <link id="stylesheet-base" rel="preload" href="/css/style.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
    <link id="stylesheet-mobile" rel="preload" href="/css/mobile.css" as="style" onload="this.onload=null;this.rel='stylesheet';this.media='screen and (max-width: 960px)'">
    <link id="stylesheet-theme-dark" rel="preload" href="/css/dark.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
    <link rel="preload" href="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" as="script">
    <link rel="preload" href="/scripts/main.js" as="script">
    <link rel="preload" href="/font/Oswald-Regular.ttf" as="font" crossorigin>
    <link rel="preload" href="https://at.alicdn.com/t/font_327081_1dta1rlogw17zaor.woff" as="font" crossorigin>
    <!-- algolia -->
    <!-- 百度统计  -->
    <!-- 谷歌统计  -->
    <!-- Google tag (gtag.js) -->
<meta name="generator" content="Hexo 7.3.0"><link href="https://cdn.bootcss.com/KaTeX/0.11.1/katex.min.css" rel="stylesheet" /></head>

    <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js"></script>
    <script type="text/javascript">
        if (typeof window.$ == undefined) {
            console.warn('jquery load from jsdelivr failed, will load local script')
            document.write('<script src="/lib/jquery.min.js" />')
        }
    </script>
        <body class="post-body">
        <!-- header -->
        <header class="header header-mobile">
    <!-- top read progress line -->
    <div class="header-element">
        <div class="read-progress"></div>
    </div>
    <!-- sidebar menu button -->
    <div class="header-element">
        <div class="header-sidebar-menu">
            <div style="padding-left: 1px;">&#xe775;</div>
        </div>
    </div>
    <!-- header actions -->
    <div class="header-actions">
        <!-- theme mode switch button -->
        <span class="header-theme-btn header-element">
            <i class="fas fa-adjust"></i>
        </span>
        <!-- back to home page text -->
        <span class="home-link header-element">
            <a href="/">BiribiriBird's Gallery.</a>
        </span>
    </div>
    <!-- toggle banner -->
    <div class="banner">
        <div class="blog-title header-element">
            <a href="/">BiribiriBird&#39;s Gallery.</a>
        </div>
        <div class="post-title header-element">
            <a href="#" class="post-name">Pytorch小土堆 - 笔记</a>
        </div>
    </div>
</header>

        <!-- fixed footer -->
        <footer class="footer-fixed">
    <!-- donate button -->

    <!-- back to top button -->
    <div class="footer-fixed-btn footer-fixed-btn--hidden back-top">
        <div>&#xe639;</div>
    </div>
</footer>

        <!-- wrapper -->
        <div class="wrapper">
            <div class="site-intro" style="    height:50vh;
">
    <!-- 主页  -->
    <!-- 404页  -->
    <div class="site-intro-placeholder"></div>
    <div class="site-intro-img" style="background-image: url(/intro/post-bg.jpg)"></div>
    <div class="site-intro-meta">
        <!-- 标题  -->
        <h1 class="intro-title">
            <!-- 主页  -->
                Pytorch小土堆 - 笔记
            <!-- 404 -->
        </h1>
        <!-- 副标题 -->
        <p class="intro-subtitle">
            <!-- 主页副标题  -->
            <!-- 404 -->
        </p>
        <!-- 文章页 meta -->
            <div class="post-intros">
                <!-- 文章页标签  -->
                    <div class="post-intro-tags" >
        <a class="post-tag" href="javascript:void(0);" data-tags="深度学习">深度学习</a>
        <a class="post-tag" href="javascript:void(0);" data-tags="Pytorch">Pytorch</a>
</div>

                <!-- 文章字数统计 -->
                    <div class="post-intro-read">
                        <span>字数统计: <span class="post-count word-count">3.7k</span>阅读时长: <span class="post-count reading-time">18 min</span></span>
                    </div>
                <div class="post-intro-meta">
                    <!-- 撰写日期 -->
                    <span class="iconfont-archer post-intro-calander">&#xe676;</span>
                    <span class="post-intro-time">2024/07/21</span>
                    <!-- busuanzi -->
                        <span id="busuanzi_container_page_pv" class="busuanzi-pv">
                            <span class="iconfont-archer post-intro-busuanzi">&#xe602;</span>
                            <span id="busuanzi_value_page_pv"></span>
                        </span>
                    <!-- 文章分享 -->
                    <span class="share-wrapper">
                        <span class="iconfont-archer share-icon">&#xe71d;</span>
                        <span class="share-text">Share</span>
                        <ul class="share-list">
                            <li class="iconfont-archer share-qr" data-type="qr">&#xe75b;
                                <div class="share-qrcode"></div>
                            </li>
                            <li class="iconfont-archer" data-type="weibo">&#xe619;</li>
                            <li class="iconfont-archer" data-type="qzone">&#xe62e;</li>
                            <li class="iconfont-archer" data-type="twitter">&#xe634;</li>
                            <li class="iconfont-archer" data-type="facebook">&#xe67a;</li>
                        </ul>
                    </span>
                </div>
            </div>
    </div>
</div>

            <script>
  // get user agent
  function getBrowserVersions() {
    var u = window.navigator.userAgent
    return {
      userAgent: u,
      trident: u.indexOf('Trident') > -1, //IE内核
      presto: u.indexOf('Presto') > -1, //opera内核
      webKit: u.indexOf('AppleWebKit') > -1, //苹果、谷歌内核
      gecko: u.indexOf('Gecko') > -1 && u.indexOf('KHTML') == -1, //火狐内核
      mobile: !!u.match(/AppleWebKit.*Mobile.*/), //是否为移动终端
      ios: !!u.match(/\(i[^;]+;( U;)? CPU.+Mac OS X/), //ios终端
      android: u.indexOf('Android') > -1 || u.indexOf('Linux') > -1, //android终端或者uc浏览器
      iPhone: u.indexOf('iPhone') > -1 || u.indexOf('Mac') > -1, //是否为iPhone或者安卓QQ浏览器
      iPad: u.indexOf('iPad') > -1, //是否为iPad
      webApp: u.indexOf('Safari') == -1, //是否为web应用程序，没有头部与底部
      weixin: u.indexOf('MicroMessenger') == -1, //是否为微信浏览器
      uc: u.indexOf('UCBrowser') > -1, //是否为android下的UC浏览器
    }
  }
  var browser = {
    versions: getBrowserVersions(),
  }
  console.log('userAgent: ' + browser.versions.userAgent)

  // callback
  function fontLoaded() {
    console.log('font loaded')
    if (document.getElementsByClassName('site-intro-meta')) {
      document
        .getElementsByClassName('intro-title')[0]
        .classList.add('intro-fade-in')
      document
        .getElementsByClassName('intro-subtitle')[0]
        .classList.add('intro-fade-in')
      var postIntros = document.getElementsByClassName('post-intros')[0]
      if (postIntros) {
        postIntros.classList.add('post-fade-in')
      }
    }
  }

  // UC不支持跨域，所以直接显示
  function asyncCb() {
    if (browser.versions.uc) {
      console.log('UCBrowser')
      fontLoaded()
    } else {
      WebFont.load({
        custom: {
          families: ['Oswald-Regular'],
        },
        loading: function () {
          // 所有字体开始加载
          // console.log('font loading');
        },
        active: function () {
          // 所有字体已渲染
          fontLoaded()
        },
        inactive: function () {
          // 字体预加载失败，无效字体或浏览器不支持加载
          console.log('inactive: timeout')
          fontLoaded()
        },
        timeout: 5000, // Set the timeout to two seconds
      })
    }
  }

  function asyncErr() {
    console.warn('script load from CDN failed, will load local script')
  }

  // load webfont-loader async, and add callback function
  function async(u, cb, err) {
    var d = document,
      t = 'script',
      o = d.createElement(t),
      s = d.getElementsByTagName(t)[0]
    o.src = u
    if (cb) {
      o.addEventListener(
        'load',
        function (e) {
          cb(null, e)
        },
        false
      )
    }
    if (err) {
      o.addEventListener(
        'error',
        function (e) {
          err(null, e)
        },
        false
      )
    }
    s.parentNode.insertBefore(o, s)
  }

  var asyncLoadWithFallBack = function (arr, success, reject) {
    var currReject = function () {
      reject()
      arr.shift()
      if (arr.length) async(arr[0], success, currReject)
    }

    async(arr[0], success, currReject)
  }

  asyncLoadWithFallBack(
    [
      'https://cdn.jsdelivr.net/npm/webfontloader@1.6.28/webfontloader.min.js',
      'https://cdn.bootcss.com/webfont/1.6.28/webfontloader.js',
      "/lib/webfontloader.min.js",
    ],
    asyncCb,
    asyncErr
  )
</script>

            <img class="loading" src="/assets/loading.svg" style="display: block; margin: 6rem auto 0 auto; width: 6rem; height: 6rem;" alt="loading">
            <div class="container container-unloaded">
                <main class="main post-page">
    <article class="article-entry">
        <h1><span id="pytorch入门">Pytorch入门</span></h1>
<!-- toc -->
<ul>
<li><a href="#%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE">环境配置</a></li>
<li><a href="#%E5%89%8D%E8%A8%80">前言</a>
<ul>
<li><a href="#%E6%B3%95%E5%AE%9D%E5%87%BD%E6%95%B0">法宝函数</a></li>
<li><a href="#dataset">Dataset</a></li>
<li><a href="#tensorboard">Tensorboard</a>
<ul>
<li><a href="#scalars">Scalars</a></li>
<li><a href="#images">Images</a></li>
</ul>
</li>
<li><a href="#transforms">Transforms</a></li>
<li><a href="#torchvision%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84%E4%B8%8B%E8%BD%BD%E4%B8%8E%E4%BD%BF%E7%94%A8">Torchvision数据集的下载与使用</a></li>
<li><a href="#dataloader">DataLoader</a></li>
</ul>
</li>
<li><a href="#%E7%BD%91%E7%BB%9C">网络</a>
<ul>
<li><a href="#%E5%8D%B7%E7%A7%AF%E5%B1%82">卷积层</a></li>
<li><a href="#%E6%B1%A0%E5%8C%96%E5%B1%82">池化层</a></li>
<li><a href="#%E6%BF%80%E6%B4%BB%E5%B1%82">激活层</a></li>
<li><a href="#%E5%85%B6%E4%BB%96">其他</a></li>
<li><a href="#sequential">Sequential</a></li>
</ul>
</li>
<li><a href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0">损失函数</a></li>
<li><a href="#%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD">反向传播</a></li>
<li><a href="#%E4%BC%98%E5%8C%96%E5%99%A8">优化器</a></li>
<li><a href="#%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%BF%AE%E6%94%B9">模型的修改</a>
<ul>
<li><a href="#%E6%B7%BB%E5%8A%A0">添加</a></li>
<li><a href="#%E4%BF%AE%E6%94%B9">修改</a></li>
</ul>
</li>
<li><a href="#%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%BF%9D%E5%AD%98%E4%B8%8E%E8%AF%BB%E5%8F%96">模型的保存与读取</a>
<ul>
<li><a href="#save">save</a></li>
<li><a href="#state_dict%E5%AE%98%E6%96%B9%E6%8E%A8%E8%8D%90">state_dict（官方推荐）</a></li>
</ul>
</li>
<li><a href="#gpu%E8%AE%AD%E7%BB%83">GPU训练</a></li>
<li><a href="#%E5%AE%8C%E6%95%B4%E6%B5%81%E7%A8%8B">完整流程</a></li>
</ul>
<!-- tocstop -->
<h2><span id="环境配置">环境配置</span></h2>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda create -n pytorch-learn python=3.8</span><br></pre></td></tr></table></figure>
<p>查看cuda版本：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nvcc --version</span><br></pre></td></tr></table></figure>
<p>根据cuda版本进行选择：<a target="_blank" rel="noopener" href="https://pytorch.org/get-started/locally/">Pytorch本地安装</a></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia</span><br></pre></td></tr></table></figure>
<p>安装检测</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">(pytorch-learn) aoijays@aoijays-ubuntu:~/Desktop/note$ python                                       </span><br><span class="line">Python <span class="number">3.8</span><span class="number">.19</span> (<span class="keyword">default</span>, Mar <span class="number">20</span> <span class="number">2024</span>, <span class="number">19</span>:<span class="number">58</span>:<span class="number">24</span>)                                                      </span><br><span class="line">[GCC <span class="number">11.2</span><span class="number">.0</span>] :: Anaconda, Inc. on linux                                                             </span><br><span class="line">Type <span class="string">&quot;help&quot;</span>, <span class="string">&quot;copyright&quot;</span>, <span class="string">&quot;credits&quot;</span> <span class="keyword">or</span> <span class="string">&quot;license&quot;</span> <span class="keyword">for</span> more information.                              </span><br><span class="line">&gt;&gt;&gt; <span class="keyword">import</span> torch                                                                                     </span><br><span class="line">&gt;&gt;&gt; torch.cuda.<span class="built_in">is_available</span>()</span><br><span class="line">True        </span><br></pre></td></tr></table></figure>
<p>此时说明安装成功</p>
<h2><span id="前言">前言</span></h2>
<h3><span id="法宝函数">法宝函数</span></h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">dir</span>( ... ) <span class="comment"># 显示下属对象列表</span></span><br><span class="line"><span class="built_in">help</span>( ... ) <span class="comment"># 显示当前对象的说明</span></span><br><span class="line"><span class="comment">## 但我更喜欢加两个问号??</span></span><br></pre></td></tr></table></figure>
<p><img src="/pic/Pytorch_%E5%B0%8F%E5%9C%9F%E5%A0%86/image-20240529020547606.png" alt="image-20240529020547606"></p>
<h3><span id="dataset">Dataset</span></h3>
<p>以<a target="_blank" rel="noopener" href="https://www.kaggle.com/datasets/ajayrana/hymenoptera-data">蜜蜂蚂蚁数据集</a>进行说明</p>
<p>其文件目录：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">│   └── hymenoptera_data</span><br><span class="line">│       ├── train</span><br><span class="line">│       │   ├── ants</span><br><span class="line">│       │   └── bees</span><br><span class="line">│       └── val</span><br><span class="line">│           ├── ants</span><br><span class="line">│           └── bees</span><br></pre></td></tr></table></figure>
<p>子文件夹小附有若干张jpg</p>
<p>接下来我们需要使用torch的Dataset去加载数据集</p>
<blockquote>
<p>|  An abstract class representing a :class:<code>Dataset</code>.<br>
|<br>
|  All datasets that represent <u><strong>a map from keys to data samples</strong></u> should subclass<br>
|  it. All subclasses should overwrite :meth:<code>__getitem__</code>, supporting fetching a<br>
|  data sample for a given key. Subclasses could also optionally overwrite<br>
|  :meth:<code>__len__</code>, which is expected to return the size of the dataset by many<br>
|  :class:<code>~torch.utils.data.Sampler</code> implementations and the default options<br>
|  of :class:<code>~torch.utils.data.DataLoader</code>. Subclasses could also<br>
|  optionally implement :meth:<code>__getitems__</code>, for speedup batched samples<br>
|  loading. This method accepts list of indices of samples of batch and returns<br>
|  list of samples.</p>
</blockquote>
<p>省流：</p>
<ul>
<li>需要继承</li>
<li>必须重写<code>__getitem__</code></li>
<li>可以重写<code>__len__</code></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyDataet</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, root_dir, label</span>):</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 自定义 怎么方便怎么来</span></span><br><span class="line">        <span class="variable language_">self</span>.root_dir = root_dir <span class="comment"># 记录数据地址以及对应的标签</span></span><br><span class="line">        <span class="variable language_">self</span>.label = label</span><br><span class="line">        <span class="variable language_">self</span>.imglist = os.listdir(<span class="variable language_">self</span>.root_dir) <span class="comment"># 以列表形式展示文件夹内所有文件</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        img_name= <span class="variable language_">self</span>.imglist[idx]</span><br><span class="line">        img_path = os.path.join( <span class="variable language_">self</span>.root_dir, img_name )</span><br><span class="line">        </span><br><span class="line">        img = Image.<span class="built_in">open</span>(img_path) <span class="comment"># 打开图片</span></span><br><span class="line">        label = <span class="variable language_">self</span>.label</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> img, label <span class="comment"># 返回数据与标签</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(<span class="variable language_">self</span>.imglist)</span><br><span class="line">    </span><br><span class="line">train_ants = MyDataet(<span class="string">&#x27;../Dataset/hymenoptera_data/train/ants&#x27;</span>, <span class="string">&#x27;ants&#x27;</span>)  </span><br><span class="line">train_bees = MyDataet(<span class="string">&#x27;../Dataset/hymenoptera_data/train/bees&#x27;</span>, <span class="string">&#x27;bees&#x27;</span>)  </span><br><span class="line"></span><br><span class="line">train_data = train_ants + train_bees <span class="comment"># 拼接数据集</span></span><br><span class="line"><span class="built_in">print</span>( train_ants.__len__(),  train_bees.__len__(), train_data.__len__())</span><br><span class="line"><span class="built_in">print</span>(train_data[<span class="number">123</span>], train_data[<span class="number">124</span>], sep=<span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line"><span class="comment"># 124 121 245</span></span><br><span class="line"><span class="comment"># (&lt;PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=500x375 at 0x775FB149ADF0&gt;, &#x27;ants&#x27;)</span></span><br><span class="line"><span class="comment"># (&lt;PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=311x387 at 0x775FB0EB29A0&gt;, &#x27;bees&#x27;)</span></span><br></pre></td></tr></table></figure>
<p>写法非常自由，你只要保证重写的函数返回正确结果即可</p>
<h3><span id="tensorboard">Tensorboard</span></h3>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda install tensorboard</span><br></pre></td></tr></table></figure>
<h4><span id="scalars">Scalars</span></h4>
<p>绘制一些图表，观察训练的loss变化</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line">writer = SummaryWriter(<span class="string">&quot;logs&quot;</span>) <span class="comment"># 创建对象 在logs文件夹下保存文件</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    writer.add_scalar(<span class="string">&quot;y = x&quot;</span>, i, i) <span class="comment"># 图像名 y值 x值</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    writer.add_scalar(<span class="string">&quot;y = x^2&quot;</span>, i*i, i) <span class="comment"># 图像名 y值 x值</span></span><br><span class="line"></span><br><span class="line">writer.close()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>在终端中：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensorboard --logdir=logs --port=6007 <span class="comment"># 默认6006</span></span><br></pre></td></tr></table></figure>
<p><img src="/pic/Pytorch_%E5%B0%8F%E5%9C%9F%E5%A0%86/image-20240529030116600.png" alt="image-20240529030116600"></p>
<h4><span id="images">Images</span></h4>
<p>可视化实际的训练效果，上传图片进行展示</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">img = Image.<span class="built_in">open</span>(</span><br><span class="line">    <span class="string">&#x27;../Dataset/hymenoptera_data/train/ants/0013035.jpg&#x27;</span></span><br><span class="line">)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(img)) <span class="comment"># &lt;class &#x27;PIL.JpegImagePlugin.JpegImageFile&#x27;&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 不支持此类型 需要转化为numpy对象</span></span><br><span class="line">img_array = np.array(img)</span><br><span class="line"><span class="built_in">print</span>(img_array.shape) <span class="comment"># (512, 768, 3)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 高 宽 通道数 -&gt; HWC</span></span><br><span class="line"><span class="comment"># 标题 添加的图像 步 格式</span></span><br><span class="line">writer.add_image(<span class="string">&quot;test&quot;</span>, img_array, <span class="number">1</span>, dataformats=<span class="string">&quot;HWC&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 我们可以第1步展示一张图，第2步展示一张图……</span></span><br><span class="line"><span class="comment"># 即可观察随着训练步数，图片发生变化</span></span><br></pre></td></tr></table></figure>
<p><img src="/pic/Pytorch_%E5%B0%8F%E5%9C%9F%E5%A0%86/image-20240529031540127.png" alt="image-20240529031540127"></p>
<h3><span id="transforms">Transforms</span></h3>
<p>留个印象就好，能通过这个方法对数据、图片等进行互相转换、变化</p>
<p>常见的有：</p>
<ul>
<li>ToTensor：转化为Tensor数据</li>
<li>Normalize：归一化</li>
<li>Resize：对图片数据进行缩放</li>
<li>Compose：用列表记录多个变化，进行一次性操作</li>
</ul>
<p>可能用到的时候查一下就行</p>
<p>适合对多个数据同时进行相同的处理</p>
<h3><span id="torchvision数据集的下载与使用">Torchvision数据集的下载与使用</span></h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line">train_set = torchvision.datasets.CIFAR10(root=<span class="string">&#x27;../Dataset&#x27;</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>) <span class="comment"># 训练集</span></span><br><span class="line">test_set = torchvision.datasets.CIFAR10(root=<span class="string">&#x27;../Dataset&#x27;</span>, train=<span class="literal">False</span>, download=<span class="literal">True</span>) <span class="comment"># 测试集</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(train_set[<span class="number">0</span>])</span><br><span class="line"><span class="built_in">print</span>(train_set.classes)</span><br><span class="line">img, target = test_set[<span class="number">0</span>]</span><br><span class="line"><span class="built_in">print</span>(img, target)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">(&lt;PIL.Image.Image image mode=RGB size=32x32 at 0x7202D3007670&gt;, 6)</span></span><br><span class="line"><span class="string">[&#x27;airplane&#x27;, &#x27;automobile&#x27;, &#x27;bird&#x27;, &#x27;cat&#x27;, &#x27;deer&#x27;, &#x27;dog&#x27;, &#x27;frog&#x27;, &#x27;horse&#x27;, &#x27;ship&#x27;, &#x27;truck&#x27;]</span></span><br><span class="line"><span class="string">&lt;PIL.Image.Image image mode=RGB size=32x32 at 0x7202D3007310&gt; 3</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<p>此时我们得到的数据集都是<code>(PIL对象，标签索引)</code></p>
<p>我们可以使用Transform统一把图片转化为Tensor，方便Pytorch使用</p>
<p>最简单的方法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">dataset_transform = torchvision.transforms.Compose([ <span class="comment"># 封装所有需要进行的转换列表</span></span><br><span class="line">    torchvision.transforms.ToTensor()</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在加载数据时直接进行参数化修改</span></span><br><span class="line">train_set = torchvision.datasets.CIFAR10(root=<span class="string">&#x27;../Dataset&#x27;</span>, train=<span class="literal">True</span>, transform=dataset_transform, download=<span class="literal">True</span>) <span class="comment"># 训练集</span></span><br><span class="line">test_set = torchvision.datasets.CIFAR10(root=<span class="string">&#x27;../Dataset&#x27;</span>, train=<span class="literal">False</span>, transform=dataset_transform, download=<span class="literal">True</span>) <span class="comment"># 测试集</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3><span id="dataloader">DataLoader</span></h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"></span><br><span class="line">test_set = torchvision.datasets.CIFAR10(root=<span class="string">&#x27;../Dataset&#x27;</span>, train=<span class="literal">False</span>, transform=torchvision.transforms.ToTensor(), download=<span class="literal">True</span>) <span class="comment"># 测试集</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 除了官方数据集 也可以选择之前自己实例化的Dataset</span></span><br><span class="line">test_loader = DataLoader(dataset=test_set, batch_size=<span class="number">64</span>, shuffle=<span class="literal">True</span>, drop_last=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> test_loader: <span class="comment"># 按batch_size数量进行遍历</span></span><br><span class="line">    imgs, targets = data</span><br><span class="line">    <span class="built_in">print</span>(targets)</span><br></pre></td></tr></table></figure>
<h2><span id="网络">网络</span></h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 需要从nn.Module进行继承</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Mynn</span>(nn.Module):</span><br><span class="line">    </span><br><span class="line">	<span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">		<span class="built_in">super</span>().__init__() <span class="comment"># 调用父类的初始化函数</span></span><br><span class="line">   </span><br><span class="line">	<span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span></span>): <span class="comment"># 前向传播</span></span><br><span class="line">		output = <span class="built_in">input</span> + <span class="number">1</span></span><br><span class="line">		<span class="keyword">return</span> output</span><br><span class="line">        </span><br><span class="line"></span><br><span class="line">mynn = Mynn()</span><br><span class="line">x = torch.tensor(<span class="number">1.0</span>)</span><br><span class="line">output = mynn(x)</span><br><span class="line"><span class="built_in">print</span>(output)</span><br></pre></td></tr></table></figure>
<h3><span id="卷积层">卷积层</span></h3>
<p>对于一张H*W的RGB图片，其通道数channel为3</p>
<p>我们使用多少个卷积核，就会产生多少个out_channels</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Mycnn</span>(nn.Module):</span><br><span class="line">    </span><br><span class="line">	<span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">		<span class="built_in">super</span>().__init__() </span><br><span class="line">        <span class="comment"># 输入3通道 用6个3*3的卷积核得到6通道输出 步长为1 不填充</span></span><br><span class="line">		<span class="variable language_">self</span>.conv1 = nn.Conv2d(in_channels=<span class="number">3</span>, out_channels=<span class="number">6</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">	<span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span></span>): <span class="comment"># 前向传播</span></span><br><span class="line">		output = <span class="variable language_">self</span>.conv1(<span class="built_in">input</span>)</span><br><span class="line">		<span class="keyword">return</span> output</span><br><span class="line">        </span><br><span class="line">mycnn = Mycnn()</span><br><span class="line">x = torch.FloatTensor([</span><br><span class="line">	[[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>],[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>],[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>],[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>],[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>]],</span><br><span class="line">	[[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>],[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>],[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>],[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>],[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>]],</span><br><span class="line">	[[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>],[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>],[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>],[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>],[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>]]</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">mycnn(x)</span><br></pre></td></tr></table></figure>
<h3><span id="池化层">池化层</span></h3>
<p>卷积后的图像依旧比较大，可以通过池化层进行压缩</p>
<blockquote>
<p>避免过拟合、去除冗余</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 池化</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Mycnn</span>(nn.Module):</span><br><span class="line">    </span><br><span class="line">	<span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">		<span class="built_in">super</span>().__init__() </span><br><span class="line">		<span class="comment"># ceil_mode 若有无法整除kernel_size 多余的部分按照ceil_mode决定是否保留</span></span><br><span class="line">		<span class="variable language_">self</span>.maxpool1 = nn.MaxPool2d( kernel_size=<span class="number">3</span>, ceil_mode=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">	<span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span></span>): <span class="comment"># 前向传播</span></span><br><span class="line">		output = <span class="variable language_">self</span>.maxpool1(<span class="built_in">input</span>)</span><br><span class="line">		<span class="keyword">return</span> output</span><br><span class="line">        </span><br><span class="line">mycnn = Mycnn()</span><br><span class="line">x = torch.tensor([</span><br><span class="line">	[[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>],[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>],[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>],[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>],[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>]],</span><br><span class="line">	[[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>],[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>],[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>],[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>],[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>]],</span><br><span class="line">	[[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>],[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>],[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>],[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>],[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>]]</span><br><span class="line">], dtype = torch.float32)</span><br><span class="line"></span><br><span class="line"><span class="comment"># x.shape</span></span><br><span class="line">mycnn(x)</span><br></pre></td></tr></table></figure>
<p>我们可以喂入图片，就可以得到压缩画质版本的输出</p>
<h3><span id="激活层">激活层</span></h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ReLU</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Mynn</span>(nn.Module):</span><br><span class="line">    </span><br><span class="line">	<span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">		<span class="built_in">super</span>().__init__()</span><br><span class="line">		<span class="variable language_">self</span>.ReLU = nn.ReLU()</span><br><span class="line">	</span><br><span class="line">	<span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span></span>): <span class="comment"># 前向传播</span></span><br><span class="line">		output = <span class="variable language_">self</span>.ReLU(<span class="built_in">input</span>)</span><br><span class="line">		<span class="keyword">return</span> output</span><br><span class="line">        </span><br><span class="line">mynn = Mynn()</span><br><span class="line"></span><br><span class="line"><span class="built_in">input</span> = torch.tensor([</span><br><span class="line">	[<span class="number">1.</span>, -<span class="number">0.5</span>],</span><br><span class="line">	[-<span class="number">1.</span>, <span class="number">3</span>]</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="built_in">input</span> = torch.reshape(<span class="built_in">input</span>, (-<span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">input</span>)</span><br><span class="line">output = mynn(<span class="built_in">input</span>)</span><br><span class="line"><span class="built_in">print</span>(output)</span><br></pre></td></tr></table></figure>
<h3><span id="其他">其他</span></h3>
<ul>
<li>正则化层</li>
<li>线性层……</li>
</ul>
<h3><span id="sequential">Sequential</span></h3>
<p>我们试图构建一个较大的网络对CIFAR10数据集进行推理</p>
<p><img src="/pic/Pytorch_%E5%B0%8F%E5%9C%9F%E5%A0%86/image-20240530152550478.png" alt="image-20240530152550478"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Sequential</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Mynn</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.sequential = nn.Sequential(</span><br><span class="line">            nn.Conv2d(in_channels=<span class="number">3</span>, out_channels=<span class="number">32</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            nn.Conv2d(in_channels=<span class="number">32</span>, out_channels=<span class="number">32</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            nn.Conv2d(in_channels=<span class="number">32</span>, out_channels=<span class="number">64</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            nn.Flatten(), <span class="comment"># 将tensor张成一维张量</span></span><br><span class="line">            nn.Linear(<span class="number">1024</span>, <span class="number">64</span>),</span><br><span class="line">            nn.Linear(<span class="number">64</span>, <span class="number">10</span>)</span><br><span class="line">        )</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = <span class="variable language_">self</span>.sequential(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">mynn = Mynn()</span><br><span class="line"><span class="built_in">print</span>(mynn)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模拟一个batch_size = 64中32*32的3通道数据集</span></span><br><span class="line"><span class="built_in">input</span> = torch.ones( (<span class="number">64</span>, <span class="number">3</span>, <span class="number">32</span>, <span class="number">32</span>) )</span><br><span class="line">output = mynn(<span class="built_in">input</span>)</span><br><span class="line"><span class="built_in">print</span>(output.shape)</span><br></pre></td></tr></table></figure>
<h2><span id="损失函数">损失函数</span></h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line">inputs = torch.tensor([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>], dtype=torch.float32).reshape([-<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">3</span>])</span><br><span class="line">targets = torch.tensor([<span class="number">1</span>,<span class="number">2</span>,<span class="number">5</span>], dtype=torch.float32).reshape([-<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">3</span>])</span><br><span class="line"></span><br><span class="line">loss1 = L1Loss(reduction=<span class="string">&#x27;sum&#x27;</span>)</span><br><span class="line">loss2 = L1Loss(reduction=<span class="string">&#x27;mean&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>( loss1(inputs, targets) )</span><br><span class="line"><span class="built_in">print</span>( loss2(inputs, targets) )</span><br><span class="line"></span><br><span class="line">loss3 = MSELoss()</span><br><span class="line"><span class="built_in">print</span>( loss3(inputs, targets) )</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 交叉熵损失函数</span></span><br><span class="line"><span class="comment"># 假设 outputs 是模型的最后一层输出，shape 为 (batch_size, num_classes)，targets 是 ground truth labels</span></span><br><span class="line">outputs = torch.randn(<span class="number">10</span>, <span class="number">4</span>)  <span class="comment"># 对于4分类问题的10个样本的不归一化的预测值</span></span><br><span class="line">targets = torch.randint(<span class="number">0</span>, <span class="number">4</span>, (<span class="number">10</span>,))  <span class="comment"># 对应的真实类别</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(outputs)</span><br><span class="line"><span class="built_in">print</span>(targets)</span><br><span class="line"></span><br><span class="line">loss_fn = nn.CrossEntropyLoss()</span><br><span class="line">loss = loss_fn(outputs, targets)</span><br><span class="line"><span class="built_in">print</span>(loss.item())</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2><span id="反向传播">反向传播</span></h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 构建网络</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Mynn</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.sequential = nn.Sequential(</span><br><span class="line">            nn.Conv2d(in_channels=<span class="number">3</span>, out_channels=<span class="number">32</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            nn.Conv2d(in_channels=<span class="number">32</span>, out_channels=<span class="number">32</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            nn.Conv2d(in_channels=<span class="number">32</span>, out_channels=<span class="number">64</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            nn.Flatten(), <span class="comment"># 将tensor张成一维张量</span></span><br><span class="line">            nn.Linear(<span class="number">1024</span>, <span class="number">64</span>),</span><br><span class="line">            nn.Linear(<span class="number">64</span>, <span class="number">10</span>)</span><br><span class="line">        )</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = <span class="variable language_">self</span>.sequential(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">mynn = Mynn()</span><br><span class="line"><span class="built_in">print</span>(mynn)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模拟一个batch_size = 64中32*32的3通道数据集</span></span><br><span class="line"><span class="built_in">input</span> = torch.ones( (<span class="number">64</span>, <span class="number">3</span>, <span class="number">32</span>, <span class="number">32</span>) )</span><br><span class="line">output = mynn(<span class="built_in">input</span>)</span><br><span class="line"><span class="built_in">print</span>(output.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># -------------------------------------------</span></span><br><span class="line"><span class="comment"># 构建数据集</span></span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"></span><br><span class="line">test_set = torchvision.datasets.CIFAR10(root=<span class="string">&#x27;../Dataset&#x27;</span>, train=<span class="literal">False</span>, transform=torchvision.transforms.ToTensor(), download=<span class="literal">True</span>) <span class="comment"># 测试集</span></span><br><span class="line"></span><br><span class="line">test_loader = DataLoader(dataset=test_set, batch_size=<span class="number">64</span>, shuffle=<span class="literal">True</span>, drop_last=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># -------------------------------------------</span></span><br><span class="line"><span class="comment"># 反向传播</span></span><br><span class="line"></span><br><span class="line">loss_fn = nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> test_loader: <span class="comment"># 按batch_size数量进行遍历</span></span><br><span class="line">    imgs, targets = data</span><br><span class="line">    outputs = mynn(imgs)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 计算得到损失函数</span></span><br><span class="line">    loss = loss_fn(outputs, targets)</span><br><span class="line">    loss.backward() <span class="comment"># 反向传播 沿计算图得到所有参数的梯度</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2><span id="优化器">优化器</span></h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">loss_fn = nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line">loss_sum = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> test_loader: <span class="comment"># 按batch_size数量进行遍历</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 正向推理</span></span><br><span class="line">    imgs, targets = data</span><br><span class="line">    outputs = mynn(imgs)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 计算得到损失函数</span></span><br><span class="line">    loss = loss_fn(outputs, targets)</span><br><span class="line"></span><br><span class="line">    optim.zero_grad() <span class="comment"># 将所有参数进行梯度清零</span></span><br><span class="line">    loss.backward() <span class="comment"># 反向传播 沿计算图得到所有参数的梯度</span></span><br><span class="line">    optim.step() <span class="comment"># 优化</span></span><br><span class="line"></span><br><span class="line">    loss_sum += loss</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(loss_sum / <span class="built_in">len</span>(test_loader))</span><br></pre></td></tr></table></figure>
<p>我们可以进行多轮学习</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">loss_fn = nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line">epochs = <span class="number">20</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    loss_sum = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> test_loader: <span class="comment"># 按batch_size数量进行遍历</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 正向推理</span></span><br><span class="line">        imgs, targets = data</span><br><span class="line">        outputs = mynn(imgs)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算得到损失函数</span></span><br><span class="line">        loss = loss_fn(outputs, targets)</span><br><span class="line"></span><br><span class="line">        optim.zero_grad() <span class="comment"># 将所有参数进行梯度清零</span></span><br><span class="line">        loss.backward() <span class="comment"># 反向传播 沿计算图得到所有参数的梯度</span></span><br><span class="line">        optim.step() <span class="comment"># 优化</span></span><br><span class="line"></span><br><span class="line">        loss_sum += loss</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;step&#x27;</span>, epoch, <span class="string">&#x27; = &#x27;</span>, loss_sum / <span class="built_in">len</span>(test_loader))</span><br></pre></td></tr></table></figure>
<p>当loss收敛后，就完成了训练</p>
<h2><span id="模型的修改">模型的修改</span></h2>
<p>除了自己的模型，其实也能修改别人训练完的模型</p>
<p>以此网络为例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Mynn</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.sequential1 = nn.Sequential(</span><br><span class="line">            nn.Conv2d(in_channels=<span class="number">3</span>, out_channels=<span class="number">32</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            nn.Conv2d(in_channels=<span class="number">32</span>, out_channels=<span class="number">32</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            nn.Conv2d(in_channels=<span class="number">32</span>, out_channels=<span class="number">64</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            nn.Flatten(), <span class="comment"># 将tensor张成一维张量</span></span><br><span class="line">            nn.Linear(<span class="number">1024</span>, <span class="number">64</span>),</span><br><span class="line">            nn.Linear(<span class="number">64</span>, <span class="number">10</span>)</span><br><span class="line">        )</span><br><span class="line">        <span class="variable language_">self</span>.sequential2 = nn.Linear(<span class="number">10</span>,<span class="number">10</span>)</span><br><span class="line">        <span class="variable language_">self</span>.classfication = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">10</span>,<span class="number">10</span>),</span><br><span class="line">            nn.Linear(<span class="number">10</span>,<span class="number">10</span>),</span><br><span class="line">            nn.Linear(<span class="number">10</span>,<span class="number">10</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">mynn = Mynn()</span><br><span class="line"><span class="built_in">print</span>(mynn)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>网络结构为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">Mynn(</span><br><span class="line">  (sequential1): Sequential(</span><br><span class="line">    (<span class="number">0</span>): Conv2d(<span class="number">3</span>, <span class="number">32</span>, kernel_size=(<span class="number">5</span>, <span class="number">5</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">    (<span class="number">1</span>): MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>, padding=<span class="number">0</span>, dilation=<span class="number">1</span>, ceil_mode=<span class="literal">False</span>)</span><br><span class="line">    (<span class="number">2</span>): Conv2d(<span class="number">32</span>, <span class="number">32</span>, kernel_size=(<span class="number">5</span>, <span class="number">5</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">    (<span class="number">3</span>): MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>, padding=<span class="number">0</span>, dilation=<span class="number">1</span>, ceil_mode=<span class="literal">False</span>)</span><br><span class="line">    (<span class="number">4</span>): Conv2d(<span class="number">32</span>, <span class="number">64</span>, kernel_size=(<span class="number">5</span>, <span class="number">5</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">    (<span class="number">5</span>): MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>, padding=<span class="number">0</span>, dilation=<span class="number">1</span>, ceil_mode=<span class="literal">False</span>)</span><br><span class="line">    (<span class="number">6</span>): Flatten(start_dim=<span class="number">1</span>, end_dim=-<span class="number">1</span>)</span><br><span class="line">    (<span class="number">7</span>): Linear(in_features=<span class="number">1024</span>, out_features=<span class="number">64</span>, bias=<span class="literal">True</span>)</span><br><span class="line">    (<span class="number">8</span>): Linear(in_features=<span class="number">64</span>, out_features=<span class="number">10</span>, bias=<span class="literal">True</span>)</span><br><span class="line">  )</span><br><span class="line">  (sequential2): Linear(in_features=<span class="number">10</span>, out_features=<span class="number">10</span>, bias=<span class="literal">True</span>)</span><br><span class="line">  (classfication): Sequential(</span><br><span class="line">    (<span class="number">0</span>): Linear(in_features=<span class="number">10</span>, out_features=<span class="number">10</span>, bias=<span class="literal">True</span>)</span><br><span class="line">    (<span class="number">1</span>): Linear(in_features=<span class="number">10</span>, out_features=<span class="number">10</span>, bias=<span class="literal">True</span>)</span><br><span class="line">    (<span class="number">2</span>): Linear(in_features=<span class="number">10</span>, out_features=<span class="number">10</span>, bias=<span class="literal">True</span>)</span><br><span class="line">  )</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h3><span id="添加">添加</span></h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">mynn.add_module(<span class="string">&#x27;add_ReLU&#x27;</span>, nn.ReLU() )</span><br><span class="line"><span class="built_in">print</span>(mynn)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Mynn(</span></span><br><span class="line"><span class="string">  (sequential1): Sequential(</span></span><br><span class="line"><span class="string">    (0): Conv2d(3, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))</span></span><br><span class="line"><span class="string">    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)</span></span><br><span class="line"><span class="string">    (2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))</span></span><br><span class="line"><span class="string">    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)</span></span><br><span class="line"><span class="string">    (4): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))</span></span><br><span class="line"><span class="string">    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)</span></span><br><span class="line"><span class="string">    (6): Flatten(start_dim=1, end_dim=-1)</span></span><br><span class="line"><span class="string">    (7): Linear(in_features=1024, out_features=64, bias=True)</span></span><br><span class="line"><span class="string">    (8): Linear(in_features=64, out_features=10, bias=True)</span></span><br><span class="line"><span class="string">  )</span></span><br><span class="line"><span class="string">  (sequential2): Linear(in_features=10, out_features=10, bias=True)</span></span><br><span class="line"><span class="string">  (classficatipm): Sequential(</span></span><br><span class="line"><span class="string">    (0): Linear(in_features=10, out_features=10, bias=True)</span></span><br><span class="line"><span class="string">    (1): Linear(in_features=10, out_features=10, bias=True)</span></span><br><span class="line"><span class="string">    (2): Linear(in_features=10, out_features=10, bias=True)</span></span><br><span class="line"><span class="string">  )</span></span><br><span class="line"><span class="string">  (add_ReLU): ReLU()</span></span><br><span class="line"><span class="string">)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<h3><span id="修改">修改</span></h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">mynn.classfication[<span class="number">1</span>] = nn.ReLU()</span><br><span class="line"><span class="built_in">print</span>(mynn)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Mynn(</span></span><br><span class="line"><span class="string">  (sequential1): Sequential(</span></span><br><span class="line"><span class="string">    (0): Conv2d(3, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))</span></span><br><span class="line"><span class="string">    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)</span></span><br><span class="line"><span class="string">    (2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))</span></span><br><span class="line"><span class="string">    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)</span></span><br><span class="line"><span class="string">    (4): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))</span></span><br><span class="line"><span class="string">    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)</span></span><br><span class="line"><span class="string">    (6): Flatten(start_dim=1, end_dim=-1)</span></span><br><span class="line"><span class="string">    (7): Linear(in_features=1024, out_features=64, bias=True)</span></span><br><span class="line"><span class="string">    (8): Linear(in_features=64, out_features=10, bias=True)</span></span><br><span class="line"><span class="string">  )</span></span><br><span class="line"><span class="string">  (sequential2): Linear(in_features=10, out_features=10, bias=True)</span></span><br><span class="line"><span class="string">  (classfication): Sequential(</span></span><br><span class="line"><span class="string">    (0): Linear(in_features=10, out_features=10, bias=True)</span></span><br><span class="line"><span class="string">    (1): ReLU()</span></span><br><span class="line"><span class="string">    (2): Linear(in_features=10, out_features=10, bias=True)</span></span><br><span class="line"><span class="string">  )</span></span><br><span class="line"><span class="string">  (add_ReLU): ReLU()</span></span><br><span class="line"><span class="string">)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<h2><span id="模型的保存与读取">模型的保存与读取</span></h2>
<h3><span id="save">save</span></h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">torch.save(model, <span class="string">&#x27;new_model.pth&#x27;</span>)</span><br><span class="line">model = torch.load(<span class="string">&#x27;my_model.pth&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>完整保存了模型的结构与参数，数据量大</p>
<h3><span id="state_dict官方推荐">state_dict（官方推荐）</span></h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">model = MyModel() <span class="comment"># 需要保证与读取的模型是同一个类</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 以字典形式进行 更加方便</span></span><br><span class="line">torch.save(model.state_dict(), <span class="string">&#x27;model_state_dict1.pth&#x27;</span>)</span><br><span class="line"></span><br><span class="line">state_dict = torch.load(<span class="string">&#x27;model_state_dict.pth&#x27;</span>)</span><br><span class="line">model.load_state_dict(state_dict)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2><span id="gpu训练">GPU训练</span></h2>
<ul>
<li>损失函数</li>
<li>数据</li>
<li>模型</li>
</ul>
<p>三种对象直接<code>x = x.cuda()</code>即可放入GPU显存</p>
<p>但是若GPU不存在，此时代码兼容性一般</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">device = torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">x = x.to(device)</span><br></pre></td></tr></table></figure>
<p>更推荐这种写法</p>
<h2><span id="完整流程">完整流程</span></h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"></span><br><span class="line"><span class="comment"># 导入数据</span></span><br><span class="line">train_set = torchvision.datasets.CIFAR10(root=<span class="string">&#x27;../Dataset&#x27;</span>, train=<span class="literal">True</span>, transform=torchvision.transforms.ToTensor(), download=<span class="literal">True</span>) <span class="comment"># 训练集</span></span><br><span class="line">test_set = torchvision.datasets.CIFAR10(root=<span class="string">&#x27;../Dataset&#x27;</span>, train=<span class="literal">False</span>, transform=torchvision.transforms.ToTensor(), download=<span class="literal">True</span>) <span class="comment"># 测试集</span></span><br><span class="line"></span><br><span class="line">train_data_size = <span class="built_in">len</span>(train_set)</span><br><span class="line">test_data_size = <span class="built_in">len</span>(test_set)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(train_data_size, test_data_size)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成DataLoader</span></span><br><span class="line">train_loader = DataLoader(dataset=train_set, batch_size=<span class="number">64</span>, shuffle=<span class="literal">True</span>, drop_last=<span class="literal">False</span>)</span><br><span class="line">test_loader = DataLoader(dataset=test_set, batch_size=<span class="number">64</span>, shuffle=<span class="literal">False</span>, drop_last=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<p>构建网络</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Mynn</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.sequential = nn.Sequential(</span><br><span class="line">            nn.Conv2d(in_channels=<span class="number">3</span>, out_channels=<span class="number">32</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            nn.Conv2d(in_channels=<span class="number">32</span>, out_channels=<span class="number">32</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            nn.Conv2d(in_channels=<span class="number">32</span>, out_channels=<span class="number">64</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            nn.Flatten(), <span class="comment"># 将tensor张成一维张量</span></span><br><span class="line">            nn.Linear(<span class="number">1024</span>, <span class="number">64</span>),</span><br><span class="line">            nn.Linear(<span class="number">64</span>, <span class="number">10</span>)</span><br><span class="line">        )</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = <span class="variable language_">self</span>.sequential(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<p>初始化</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> optim</span><br><span class="line"></span><br><span class="line">mynn = Mynn().to(device)</span><br><span class="line">loss_fn = nn.CrossEntropyLoss().to(device)</span><br><span class="line">optim = optim.SGD(mynn.parameters(), lr = <span class="number">1e-2</span>)</span><br></pre></td></tr></table></figure>
<p>训练</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line">writer = SummaryWriter(<span class="string">&quot;logs&quot;</span>) <span class="comment"># 创建对象 在logs文件夹下保存文件</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">epochs = <span class="number">40</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">	</span><br><span class="line">	<span class="built_in">print</span>(<span class="string">&#x27;-----------第&#123;&#125;轮训练-------------&#x27;</span>.<span class="built_in">format</span>(i + <span class="number">1</span>))</span><br><span class="line">	</span><br><span class="line">	train_size = <span class="number">0</span>	</span><br><span class="line">	train_step = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">	<span class="keyword">for</span> imgs, targets <span class="keyword">in</span> train_loader:</span><br><span class="line">		</span><br><span class="line">		imgs = imgs.to(device)</span><br><span class="line">		targets = targets.to(device)</span><br><span class="line"></span><br><span class="line">		<span class="comment"># 正向推理</span></span><br><span class="line">		outputs = mynn(imgs)</span><br><span class="line">		loss = loss_fn(outputs, targets)</span><br><span class="line">		</span><br><span class="line">		<span class="comment"># 优化</span></span><br><span class="line">		optim.zero_grad()</span><br><span class="line">		loss.backward()</span><br><span class="line">		optim.step()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">		<span class="comment"># 观察损失函数	</span></span><br><span class="line">		train_size += <span class="built_in">len</span>(imgs)</span><br><span class="line">		train_step += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">		writer.add_scalar(<span class="string">&quot;Train Loss&quot;</span>, loss.item(), train_step)</span><br><span class="line">		<span class="keyword">if</span> train_step % <span class="number">250</span> == <span class="number">0</span>:</span><br><span class="line">			<span class="built_in">print</span>(<span class="string">&#x27;&#123;&#125;/&#123;&#125; Loss = &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(train_size, train_data_size, loss.item()))</span><br><span class="line"></span><br><span class="line">	</span><br><span class="line">	<span class="comment"># 测试</span></span><br><span class="line">	total_loss = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">	<span class="comment"># 上下文管理器：</span></span><br><span class="line">	<span class="comment"># with 语句是一个上下文管理器，确保在其内部的代码块在启用了 torch.no_grad() 模式下执行。</span></span><br><span class="line">	<span class="comment"># 一旦代码块执行完毕，上下文管理器会恢复之前的状态，如果之前启用了梯度计算，则重新启用。</span></span><br><span class="line"></span><br><span class="line">	<span class="comment"># torch.no_grad() 纯推理 不需要记录梯度 节省时间</span></span><br><span class="line">	<span class="keyword">with</span> torch.no_grad():</span><br><span class="line">		<span class="keyword">for</span> imgs, targets <span class="keyword">in</span> test_loader:</span><br><span class="line"></span><br><span class="line">			imgs = imgs.to(device)</span><br><span class="line">			targets = targets.to(device)</span><br><span class="line">			</span><br><span class="line">			outputs = mynn(imgs)</span><br><span class="line">			loss = loss_fn(outputs, targets)</span><br><span class="line">			total_loss += loss.item()</span><br><span class="line"></span><br><span class="line">		writer.add_scalar(<span class="string">&quot;Test Loss&quot;</span>, total_loss/test_data_size, i)</span><br><span class="line">		<span class="built_in">print</span>(<span class="string">&#x27;测试集Loss = &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(total_loss/test_data_size))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">	<span class="keyword">if</span> (i + <span class="number">1</span>) % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">		torch.save(mynn, <span class="string">&#x27;./model/train_model&#123;&#125;.pth&#x27;</span>.<span class="built_in">format</span>(i))</span><br><span class="line">	</span><br><span class="line">writer.close()	 </span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>​</p>

    </article>
    <!-- license -->
        <div class="license-wrapper">
            <p>原文作者：<a href="https://aoijays.top">BiribiriBird</a>
            <p>原文链接：<a href="https://aoijays.top/2024/07/21/Pytorch-intro/">https://aoijays.top/2024/07/21/Pytorch-intro/</a>
            <p>发表日期：<a href="https://aoijays.top/2024/07/21/Pytorch-intro/">July 21st 2024, 9:50:19 am</a>
            <p>更新日期：<a href="https://aoijays.top/2024/07/21/Pytorch-intro/">July 28th 2024, 3:53:46 pm</a>
            <p>版权声明：本文采用<a rel="license noopener" target="_blank" href="http://creativecommons.org/licenses/by-nc/4.0/">知识共享署名-非商业性使用 4.0 国际许可协议</a>进行许可</p>
        </div>
    <!-- paginator -->
    <ul class="post-paginator">
        <li class="next">
                <div class="nextSlogan">Next Post</div>
                <a href="/2024/07/23/D2l-Part1/" title="动手学深度学习 · Part1 深度学习基础">
                    <div class="nextTitle">动手学深度学习 · Part1 深度学习基础</div>
                </a>
        </li>
        <li class="previous">
                <div class="prevSlogan">Previous Post</div>
                <a href="/2024/06/13/MLP-Handwrite-2024-6/" title="手推神经网络">
                    <div class="prevTitle">手推神经网络</div>
                </a>
        </li>
    </ul>
    <!-- comment -->
        <div class="post-comment">
            <!-- 来必力 City 版安装代码 -->

            
            
            
            <!-- utteranc评论 -->

            <!-- partial('_partial/comment/changyan') -->
            <!--PC版-->

            
            
            
        </div>
    <!-- timeliness note -->
    <!-- idea from: https://hexo.fluid-dev.com/posts/hexo-injector/#%E6%96%87%E7%AB%A0%E6%97%B6%E6%95%88%E6%80%A7%E6%8F%90%E7%A4%BA -->
    <!-- Mathjax -->
</main>

                <!-- profile -->
            </div>
            <footer class="footer footer-unloaded">
    <!-- social  -->
        <div class="social">
                            <a href="mailto:954436671@qq.com" class="iconfont-archer email" title="email" ></a>
                <a href="//github.com/aoijays" class="iconfont-archer github" target="_blank" title="github"></a>
                <a href="https://www.zhihu.com/people/Chirs2002" class="iconfont-archer zhihu" target="_blank" title="zhihu"></a>
                <a href="https://www.instagram.com/biribiribird/" class="iconfont-archer instagram" target="_blank" title="instagram"></a>
                <a href="https://space.bilibili.com/498088093" class="iconfont-archer bilibili" target="_blank" title="bilibili"></a>

        </div>
    <!-- powered by Hexo  -->
    <div class="copyright">
        <span id="hexo-power">Powered by <a href="https://hexo.io/" target="_blank">Hexo</a></span><span class="iconfont-archer power">&#xe635;</span><span id="theme-info">theme <a href="https://github.com/fi3ework/hexo-theme-archer" target="_blank">Archer</a></span>
    </div>
    <!-- website approve for Chinese user -->
    <!-- 不蒜子  -->
        <div class="busuanzi-container">
                <span id="busuanzi_container_site_pv">PV: <span id="busuanzi_value_site_pv"></span> :)</span>
        </div>
</footer>

        </div>
        <!-- toc -->
            <div class="toc-wrapper toc-wrapper-loding" style=    top:50vh;
>
                <div class="toc-catalog">
                    <span class="iconfont-archer catalog-icon">&#xe613;</span><span>CATALOG</span>
                </div>
                <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">1.</span> <span class="toc-text">Pytorch入门</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">1.1.</span> <span class="toc-text">环境配置</span></a></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">1.2.</span> <span class="toc-text">前言</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-number">1.2.1.</span> <span class="toc-text">法宝函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-number">1.2.2.</span> <span class="toc-text">Dataset</span></a></li><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-number">1.2.3.</span> <span class="toc-text">Tensorboard</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link"><span class="toc-number">1.2.3.1.</span> <span class="toc-text">Scalars</span></a></li><li class="toc-item toc-level-4"><a class="toc-link"><span class="toc-number">1.2.3.2.</span> <span class="toc-text">Images</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-number">1.2.4.</span> <span class="toc-text">Transforms</span></a></li><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-number">1.2.5.</span> <span class="toc-text">Torchvision数据集的下载与使用</span></a></li><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-number">1.2.6.</span> <span class="toc-text">DataLoader</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">1.3.</span> <span class="toc-text">网络</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-number">1.3.1.</span> <span class="toc-text">卷积层</span></a></li><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-number">1.3.2.</span> <span class="toc-text">池化层</span></a></li><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-number">1.3.3.</span> <span class="toc-text">激活层</span></a></li><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-number">1.3.4.</span> <span class="toc-text">其他</span></a></li><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-number">1.3.5.</span> <span class="toc-text">Sequential</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">1.4.</span> <span class="toc-text">损失函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">1.5.</span> <span class="toc-text">反向传播</span></a></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">1.6.</span> <span class="toc-text">优化器</span></a></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">1.7.</span> <span class="toc-text">模型的修改</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-number">1.7.1.</span> <span class="toc-text">添加</span></a></li><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-number">1.7.2.</span> <span class="toc-text">修改</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">1.8.</span> <span class="toc-text">模型的保存与读取</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-number">1.8.1.</span> <span class="toc-text">save</span></a></li><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-number">1.8.2.</span> <span class="toc-text">state_dict（官方推荐）</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">1.9.</span> <span class="toc-text">GPU训练</span></a></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">1.10.</span> <span class="toc-text">完整流程</span></a></li></ol></li></ol>
            </div>
        <!-- sidebar -->
        <div class="sidebar sidebar-hide">
    <ul class="sidebar-tabs sidebar-tabs-active-0">
        <li class="sidebar-tab-archives"><span class="iconfont-archer">&#xe67d;</span><span class="tab-name">Archive</span></li>
        <li class="sidebar-tab-tags"><span class="iconfont-archer">&#xe61b;</span><span class="tab-name">Tag</span></li>
        <li class="sidebar-tab-categories"><span class="iconfont-archer">&#xe666;</span><span class="tab-name">Cate</span></li>
    </ul>
    <div class="sidebar-content sidebar-content-show-archive">
        <div class="sidebar-panel-archives">
    <!-- 在 ejs 中将 archive 按照时间排序 -->
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
    <div class="total-and-search">
        <div class="total-archive">
        Total : 19
        </div>
        <!-- search  -->
    </div>
    <div class="post-archive">
            <div class="archive-year"> 2024 </div>
            <ul class="year-list">
        <li class="archive-post-item">
            <span class="archive-post-date">09/02</span>
            <a class="archive-post-title" href="/2024/09/02/Lee-ML-L7/">李宏毅机器学习L7 · Self-Supervised Learning</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">08/30</span>
            <a class="archive-post-title" href="/2024/08/30/Lee-ML-L6/">李宏毅机器学习L6 · Generative Adversarial Network</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">08/26</span>
            <a class="archive-post-title" href="/2024/08/26/D2l-part5-1/">动手学深度学习 · Part5 Transformer</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">08/20</span>
            <a class="archive-post-title" href="/2024/08/20/D2l-part4-2/">动手学深度学习 · Part4 现代序列模型与机器翻译实践</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">08/15</span>
            <a class="archive-post-title" href="/2024/08/15/D2l-part4-1/">动手学深度学习 · Part4 序列模型 · 初步</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">08/12</span>
            <a class="archive-post-title" href="/2024/08/12/Lee-ML-L5/">李宏毅机器学习L5 · Sequence to Sequence</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">08/11</span>
            <a class="archive-post-title" href="/2024/08/11/Lee-ML-L4/">李宏毅机器学习L4 · Sequence as input</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">08/11</span>
            <a class="archive-post-title" href="/2024/08/11/langchain-zero/">Langchain入门笔记</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">08/05</span>
            <a class="archive-post-title" href="/2024/08/05/D2l-Part3-ssd/">动手学深度学习 · Part3 SSD代码实现</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">08/04</span>
            <a class="archive-post-title" href="/2024/08/04/D2l-Part3/">动手学深度学习 · Part3 计算机视觉</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">07/28</span>
            <a class="archive-post-title" href="/2024/07/28/D2l-Part2/">动手学深度学习 · Part2 卷积神经网络</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">07/27</span>
            <a class="archive-post-title" href="/2024/07/27/Lee-ML-L3/">李宏毅机器学习L3 · Image as input</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">07/25</span>
            <a class="archive-post-title" href="/2024/07/25/Lee-ML-L2/">李宏毅机器学习L2 · What to do if my network fails to train</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">07/23</span>
            <a class="archive-post-title" href="/2024/07/23/D2l-Part1/">动手学深度学习 · Part1 深度学习基础</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">07/21</span>
            <a class="archive-post-title" href="/2024/07/21/Pytorch-intro/">Pytorch小土堆 - 笔记</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">06/13</span>
            <a class="archive-post-title" href="/2024/06/13/MLP-Handwrite-2024-6/">手推神经网络</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">05/26</span>
            <a class="archive-post-title" href="/2024/05/26/cuda-zero-2024-5/">CUDA编程0基础入门笔记</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">05/01</span>
            <a class="archive-post-title" href="/2024/05/01/CS231A/">CS231A - 计算机视觉课堂笔记</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">01/06</span>
            <a class="archive-post-title" href="/2024/01/06/hello-world/">Hello World</a>
        </li>
            </ul>
    </div>
</div>

        <div class="sidebar-panel-tags">
    <div class="sidebar-tags-name">
            <span class="sidebar-tag-name" data-tags="动手学深度学习">
                <span class="iconfont-archer">&#xe606;</span>
                动手学深度学习
            </span>
            <span class="sidebar-tag-name" data-tags="深度学习">
                <span class="iconfont-archer">&#xe606;</span>
                深度学习
            </span>
            <span class="sidebar-tag-name" data-tags="CV">
                <span class="iconfont-archer">&#xe606;</span>
                CV
            </span>
            <span class="sidebar-tag-name" data-tags="李宏毅">
                <span class="iconfont-archer">&#xe606;</span>
                李宏毅
            </span>
            <span class="sidebar-tag-name" data-tags="cuda">
                <span class="iconfont-archer">&#xe606;</span>
                cuda
            </span>
            <span class="sidebar-tag-name" data-tags="Pytorch">
                <span class="iconfont-archer">&#xe606;</span>
                Pytorch
            </span>
            <span class="sidebar-tag-name" data-tags="langchain">
                <span class="iconfont-archer">&#xe606;</span>
                langchain
            </span>
    </div>
    <div class="iconfont-archer sidebar-tags-empty">&#xe678;</div>
    <div class="tag-load-fail" style="display: none; color: #ccc; font-size: 0.6rem;">
        缺失模块，请参考主题文档进行安装配置：https://github.com/fi3ework/hexo-theme-archer#%E5%AE%89%E8%A3%85%E4%B8%BB%E9%A2%98
    </div> 
    <div class="sidebar-tags-list"></div>
</div>

        <div class="sidebar-panel-categories">
    <div class="sidebar-categories-name">
    </div>
    <div class="iconfont-archer sidebar-categories-empty">&#xe678;</div>
    <div class="sidebar-categories-list"></div>
</div>

    </div>
</div>

        <!-- site-meta -->
        <script>
    var siteMetaRoot = "/"
    if (siteMetaRoot === "undefined") {
        siteMetaRoot = '/'
    }
    var siteMeta = {
        url: "https://aoijays.top",
        root: siteMetaRoot,
        author: "BiribiriBird"
    }
</script>

        <!-- import experimental options here -->
        <!-- Custom Font -->

        <!-- main func -->
        <script src="/scripts/main.js"></script>
        <!-- fancybox -->
        <script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.36/dist/fancybox/fancybox.umd.js" onload="window.Fancybox.bind('[data-fancybox]')" defer></script>
        <!-- algolia -->
        <!-- busuanzi -->
            <script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script>
        <!-- async load share.js -->
            <script src="/scripts/share.js" async></script>
        <!-- mermaid -->
    </body>
</html>
